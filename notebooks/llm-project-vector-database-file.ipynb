{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: arxiv in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (2.1.3)\n",
      "Requirement already satisfied: langchain in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (0.3.19)\n",
      "Requirement already satisfied: torch in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (2.7.0.dev20250122+cu124)\n",
      "Requirement already satisfied: transformers in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (4.49.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (10.4.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (3.2.0)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (1.25.4)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (1.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (2.32.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-community) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\panup\\appdata\\roaming\\python\\python313\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv langchain langchain-community torch transformers pillow datasets pymupdf faiss-cpu PyMuPDF requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panup\\AppData\\Local\\Temp\\ipykernel_17700\\3284503357.py:6: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  paper = next(search.results(), None)\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "def fetch_arxiv_metadata(arxiv_id):\n",
    "    \"\"\"Fetches title, authors, and abstract from arXiv using the arxiv_id.\"\"\"\n",
    "    search = arxiv.Search(id_list=[arxiv_id])\n",
    "    paper = next(search.results(), None)\n",
    "    \n",
    "    if paper:\n",
    "        metadata = {\n",
    "            \"title\": paper.title,\n",
    "            \"authors\": [author.name for author in paper.authors],\n",
    "            \"abstract\": paper.summary,\n",
    "            \"doi\": paper.doi if paper.doi else \"DOI Not Available\",\n",
    "            \"published\": paper.published,\n",
    "            \"url\": paper.entry_id\n",
    "        }\n",
    "        return metadata\n",
    "    else:\n",
    "        return {\"error\": \"Paper not found on arXiv\"}\n",
    "\n",
    "# Extracted arXiv ID from the file (0704.0001)\n",
    "arxiv_id = \"0704.0001\"\n",
    "\n",
    "# Fetch and print metadata\n",
    "metadata = fetch_arxiv_metadata(arxiv_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('title', 'Calculation of prompt diphoton production cross sections at Tevatron and LHC energies')\n",
      "('authors', ['C. Balázs', 'E. L. Berger', 'P. M. Nadolsky', 'C. -P. Yuan'])\n",
      "('abstract', 'A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.')\n",
      "('doi', '10.1103/PhysRevD.76.013009')\n",
      "('published', datetime.datetime(2007, 4, 2, 19, 18, 42, tzinfo=datetime.timezone.utc))\n",
      "('url', 'http://arxiv.org/abs/0704.0001v2')\n"
     ]
    }
   ],
   "source": [
    "print(*metadata.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/Producer', 'dvips + GPL Ghostscript GIT PRERELEASE 9.22')\n",
      "('/CreationDate', \"D:20181023213259-04'00'\")\n",
      "('/ModDate', \"D:20181023213259-04'00'\")\n",
      "('/Creator', 'LaTeX with hyperref package')\n",
      "('/Title', '')\n",
      "('/Subject', '')\n",
      "('/Author', '')\n",
      "('/Keywords', '')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "meta = PdfReader(\"tmp/0704.0001.pdf\").metadata\n",
    "print(*meta.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PATH_SAVED_QUERIES = os.path.join(\n",
    "    os.path.dirname(os.path.abspath(__file__)),\n",
    "    \"data\",\n",
    "    \"saved_queries\"\n",
    ")\n",
    "\n",
    "filename = query2filename(query)\n",
    "filepath = os.path.join(PATH_SAVED_QUERIES, filename)\n",
    "logger.info(f\"Saving response to {filepath}\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import pymupdf  \n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor, WhisperProcessor, WhisperForConditionalGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import gc\n",
    "\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures extracted and saved in extracted_figures.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pypdf import PdfReader\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the uploaded PDF file\n",
    "pdf_path = \"tmp/0704.0001.pdf\"\n",
    "output_dir = \"extracted_figures\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Open the PDF\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "# Loop through each page\n",
    "for page_num, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()  # Extract text from page\n",
    "    if not text:\n",
    "        continue\n",
    "    \n",
    "    # Search for \"Figure\" (case insensitive)\n",
    "    if re.search(r\"\\bFigure\\b\", text, re.IGNORECASE):\n",
    "        print(f\"Figure detected on page {page_num+1}\")\n",
    "\n",
    "        # Extract images from the page\n",
    "        for img_index, img in enumerate(page.images):\n",
    "            img_data = img.data\n",
    "            img_ext = img.name.split(\".\")[-1] if \".\" in img.name else \"png\"\n",
    "\n",
    "            # Save image\n",
    "            image_path = os.path.join(output_dir, f\"figure_p{page_num+1}_{img_index}.{img_ext}\")\n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(img_data)\n",
    "\n",
    "print(f\"Figures extracted and saved in {output_dir}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_metadata(metadata_path):\n",
    "    \"\"\"\n",
    "    Load arXiv metadata from the JSON file.\n",
    "    Args:\n",
    "        metadata_path (str): Path to the metadata JSON file.\n",
    "    Returns:\n",
    "        List[dict]: List of paper metadata.\n",
    "    \"\"\"\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = [json.loads(line) for line in f]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download PDFs from arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_pdf(paper_id, output_folder):\n",
    "    \"\"\"\n",
    "    Download a PDF from arXiv using the paper ID.\n",
    "    Args:\n",
    "        paper_id (str): arXiv paper ID (e.g., \"0001.0001\").\n",
    "        output_folder (str): Folder to save the downloaded PDF.\n",
    "    Returns:\n",
    "        str: Path to the downloaded PDF.\n",
    "    \"\"\"\n",
    "    pdf_url = f\"https://arxiv.org/pdf/{paper_id}.pdf\"\n",
    "    pdf_path = os.path.join(output_folder, f\"{paper_id}.pdf\")\n",
    "\n",
    "    # Download the PDF\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(pdf_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return pdf_path\n",
    "    else:\n",
    "        print(f\"Failed to download PDF for paper {paper_id}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a projection layer to normalize embedding dimensions\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=512):\n",
    "        super(ProjectionLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize projection layer for text embeddings\n",
    "projection_layer_text = ProjectionLayer(input_dim=384, output_dim=512)  # Map text embeddings to 512 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Images from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    \"\"\"\n",
    "    Extract images from a PDF file and save them to the output folder.\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        output_folder (str): Folder to save extracted images.\n",
    "    Returns:\n",
    "        List[str]: List of paths to extracted images.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    pdf_document = pymupdf.open(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    # Iterate through pages and extract images\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        # Save each image\n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image_filename = f\"page_{page_num + 1}_img_{img_index + 1}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "\n",
    "            with open(image_path, \"wb\") as image_file:\n",
    "                image_file.write(image_bytes)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def extract_all_possible_images(pdf_path, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    pdf = fitz.open(pdf_path)\n",
    "    image_count = 0\n",
    "    \n",
    "    # Method 1: Extract images using get_images()\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):\n",
    "            try:\n",
    "                xref = img[0]\n",
    "                base_image = pdf.extract_image(xref)\n",
    "                image_bytes = base_image[\"image\"]\n",
    "                image_ext = base_image[\"ext\"]\n",
    "                \n",
    "                image_count += 1\n",
    "                output_filename = f\"{output_dir}/method1_page{page_num+1}_img{img_index}.{image_ext}\"\n",
    "                \n",
    "                with open(output_filename, \"wb\") as image_file:\n",
    "                    image_file.write(image_bytes)\n",
    "                \n",
    "                print(f\"Method 1: Saved image {image_count} from page {page_num+1}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Method 1: Failed on page {page_num+1}, img {img_index}: {e}\")\n",
    "    \n",
    "    # Method 2: Extract images from page dictionaries\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        \n",
    "        try:\n",
    "            # Process XObject resources\n",
    "            if \"XObject\" in page.get_resources():\n",
    "                xobjects = page.get_resources()[\"XObject\"]\n",
    "                for key, xobject in xobjects.items():\n",
    "                    try:\n",
    "                        if xobject.get(\"Subtype\") == \"Image\":\n",
    "                            image_count += 1\n",
    "                            \n",
    "                            # Try to extract image data\n",
    "                            pix = fitz.Pixmap(pdf, xobject.get(\"SMask\", 0))\n",
    "                            output_filename = f\"{output_dir}/method2_page{page_num+1}_{key}.png\"\n",
    "                            pix.save(output_filename)\n",
    "                            print(f\"Method 2: Saved image {image_count} from page {page_num+1}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Method 2: Failed on page {page_num+1}, key {key}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2: Failed to process resources on page {page_num+1}: {e}\")\n",
    "    \n",
    "    # Method 3: Render full pages at high resolution\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        \n",
    "        try:\n",
    "            pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))\n",
    "            output_filename = f\"{output_dir}/method3_page{page_num+1}.png\"\n",
    "            pix.save(output_filename)\n",
    "            image_count += 1\n",
    "            print(f\"Method 3: Saved full page {page_num+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3: Failed to render page {page_num+1}: {e}\")\n",
    "    \n",
    "    pdf.close()\n",
    "    return image_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Saved image 1 from page 15\n",
      "Method 2: Failed to process resources on page 1: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 2: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 3: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 4: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 5: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 6: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 7: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 8: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 9: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 10: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 11: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 12: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 13: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 14: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 15: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 16: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 17: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 18: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 19: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 20: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 21: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 22: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 23: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 24: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 25: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 26: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 27: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 28: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 29: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 30: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 31: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 32: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 33: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 34: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 35: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 36: 'Page' object has no attribute 'get_resources'\n",
      "Method 2: Failed to process resources on page 37: 'Page' object has no attribute 'get_resources'\n",
      "Method 3: Saved full page 1\n",
      "Method 3: Saved full page 2\n",
      "Method 3: Saved full page 3\n",
      "Method 3: Saved full page 4\n",
      "Method 3: Saved full page 5\n",
      "Method 3: Saved full page 6\n",
      "Method 3: Saved full page 7\n",
      "Method 3: Saved full page 8\n",
      "Method 3: Saved full page 9\n",
      "Method 3: Saved full page 10\n",
      "Method 3: Saved full page 11\n",
      "Method 3: Saved full page 12\n",
      "Method 3: Saved full page 13\n",
      "Method 3: Saved full page 14\n",
      "Method 3: Saved full page 15\n",
      "Method 3: Saved full page 16\n",
      "Method 3: Saved full page 17\n",
      "Method 3: Saved full page 18\n",
      "Method 3: Saved full page 19\n",
      "Method 3: Saved full page 20\n",
      "Method 3: Saved full page 21\n",
      "Method 3: Saved full page 22\n",
      "Method 3: Saved full page 23\n",
      "Method 3: Saved full page 24\n",
      "Method 3: Saved full page 25\n",
      "Method 3: Saved full page 26\n",
      "Method 3: Saved full page 27\n",
      "Method 3: Saved full page 28\n",
      "Method 3: Saved full page 29\n",
      "Method 3: Saved full page 30\n",
      "Method 3: Saved full page 31\n",
      "Method 3: Saved full page 32\n",
      "Method 3: Saved full page 33\n",
      "Method 3: Saved full page 34\n",
      "Method 3: Saved full page 35\n",
      "Method 3: Saved full page 36\n",
      "Method 3: Saved full page 37\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TEST extract_images_from_pdf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m extract_all_possible_images(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/0704.0001.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp/images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imageName \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[0;32m      5\u001b[0m     display(Image(filename\u001b[38;5;241m=\u001b[39mimageName))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# TEST extract_images_from_pdf\n",
    "image_paths = extract_all_possible_images(\"tmp/0704.0001.pdf\", \"tmp/images\")\n",
    "\n",
    "for imageName in image_paths:\n",
    "    display(Image(filename=imageName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_and_embed_images(image_paths):\n",
    "    \"\"\"\n",
    "    Extract images and generate embeddings using OpenAI's CLIP model.\n",
    "    Args:\n",
    "        image_paths (List[str]): List of paths to images.\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of image embeddings as NumPy arrays.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    # Load CLIP model and processor\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Process images and generate embeddings\n",
    "    image_embeddings = []\n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            # Open the image and convert to RGB\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            \n",
    "            # Convert to numpy array and fix shape\n",
    "            image_np = np.array(image)\n",
    "            \n",
    "            # Fix ambiguous shapes (e.g., (1, 477, 3) → (477, 3, 3))\n",
    "            if image_np.ndim == 3 and image_np.shape[0] == 1:\n",
    "                # Remove singleton dimension (e.g., shape becomes (477, 3))\n",
    "                image_np = np.squeeze(image_np, axis=0)\n",
    "                # Replicate to create 3 channels if needed\n",
    "                if image_np.ndim == 2:\n",
    "                    image_np = np.stack([image_np] * 3, axis=-1)\n",
    "            \n",
    "            # Ensure 3 channels\n",
    "            if image_np.shape[-1] != 3:\n",
    "                image_np = np.stack([image_np[..., 0]] * 3, axis=-1)\n",
    "            \n",
    "            # Convert back to PIL Image\n",
    "            image = Image.fromarray(image_np)\n",
    "            \n",
    "            # Process the image with CLIP\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.get_image_features(**inputs)\n",
    "                \n",
    "            image_embeddings.append(image_features.cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_and_embed_text(metadata, projection_layer=None):\n",
    "    \"\"\"\n",
    "    Generate text embeddings for titles and abstracts using a sentence transformer.\n",
    "    Args:\n",
    "        metadata (List[dict]): List of paper metadata.\n",
    "        projection_layer (nn.Module): Projection layer to normalize embedding dimensions.\n",
    "    Returns:\n",
    "        List[np.ndarray]: List of text embeddings as NumPy arrays.\n",
    "    \"\"\"\n",
    "    # Load text embedding model\n",
    "    text_embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Generate text embeddings\n",
    "    text_embeddings = []\n",
    "    for paper in metadata:\n",
    "        text = f\"{paper['title']} {paper['abstract']}\"\n",
    "        embedding = text_embedder.encode(text)\n",
    "        if projection_layer:\n",
    "            embedding = torch.tensor(embedding, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                embedding = projection_layer(embedding).numpy()\n",
    "        text_embeddings.append(embedding)\n",
    "\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings, output_folder, batch_id, prefix):\n",
    "    \"\"\"\n",
    "    Save embeddings to disk as a NumPy file.\n",
    "    Args:\n",
    "        embeddings (List[np.ndarray]): List of embeddings.\n",
    "        output_folder (str): Folder to save embeddings.\n",
    "        batch_id (int): Batch ID for the filename.\n",
    "        prefix (str): Prefix for the filename (e.g., \"image\" or \"text\").\n",
    "    \"\"\"\n",
    "    # Convert embeddings to a 2D NumPy array\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Save embeddings to disk\n",
    "    embeddings_path = os.path.join(output_folder, f\"{prefix}_embeddings_batch_{batch_id}.npy\")\n",
    "    np.save(embeddings_path, embeddings_array)\n",
    "    print(f\"Saved {prefix} embeddings to {embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Save Faiss Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_faiss_index(embeddings, projection_layer=None):\n",
    "    \"\"\"\n",
    "    Build a FAISS index from the embeddings.\n",
    "    Args:\n",
    "        embeddings (List[np.ndarray]): List of embeddings.\n",
    "        projection_layer (nn.Module): Projection layer to normalize embedding dimensions.\n",
    "    Returns:\n",
    "        faiss.Index: FAISS index.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to a 2D NumPy array\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Apply projection layer if provided\n",
    "    if projection_layer:\n",
    "        embeddings_array = torch.tensor(embeddings_array, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            embeddings_array = projection_layer(embeddings_array).numpy()\n",
    "\n",
    "    # Build FAISS index\n",
    "    dimension = embeddings_array.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_array)\n",
    "\n",
    "    return index\n",
    "\n",
    "# Save FAISS Index and Metadata\n",
    "def save_faiss_index_and_metadata(index, id_to_doc, output_folder):\n",
    "    \"\"\"\n",
    "    Save the FAISS index and id_to_doc mappings.\n",
    "    Args:\n",
    "        index (faiss.Index): FAISS index.\n",
    "        id_to_doc (dict): Mapping from IDs to document metadata.\n",
    "        output_folder (str): Folder to save the index and metadata.\n",
    "    \"\"\"\n",
    "    # Save FAISS index\n",
    "    faiss_index_path = os.path.join(output_folder, \"faiss_index.index\")\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(f\"Saved FAISS index to {faiss_index_path}\")\n",
    "\n",
    "    # Save id_to_doc mappings\n",
    "    id_to_doc_path = os.path.join(output_folder, \"id_to_doc.json\")\n",
    "    with open(id_to_doc_path, \"w\") as f:\n",
    "        json.dump(id_to_doc, f)\n",
    "    print(f\"Saved id_to_doc mappings to {id_to_doc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_papers(metadata, output_base, num_papers=500, batch_size=10):\n",
    "    # Create output directories\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    pdf_dir = os.path.join(output_base, \"pdfs\")\n",
    "    img_dir = os.path.join(output_base, \"images\")\n",
    "    emb_dir = os.path.join(output_base, \"embeddings\")\n",
    "    index_dir = os.path.join(output_base, \"faiss_index\")\n",
    "    \n",
    "    for d in [pdf_dir, img_dir, emb_dir, index_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    # Initialize models\n",
    "    text_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    projection = ProjectionLayer(384, 512).eval()\n",
    "    \n",
    "    # Initialize FAISS and metadata\n",
    "    id_to_doc = {}\n",
    "    global_idx = 0\n",
    "    index = faiss.IndexFlatL2(512)\n",
    "    \n",
    "    # Process papers\n",
    "    for batch_idx in tqdm(range(0, num_papers, batch_size)):\n",
    "        batch = metadata[batch_idx:batch_idx+batch_size]\n",
    "        \n",
    "        for paper in batch:\n",
    "            paper_id = paper['id']\n",
    "            pdf_path = download_pdf(paper_id, pdf_dir)\n",
    "            if not pdf_path:\n",
    "                continue\n",
    "\n",
    "            # Text Embedding\n",
    "            text = f\"{paper['title']} {paper['abstract']}\"\n",
    "            text_emb = text_encoder.encode(text)\n",
    "            with torch.no_grad():\n",
    "                proj_emb = projection(torch.tensor(text_emb)).numpy()\n",
    "            \n",
    "            # Add to index and map ID\n",
    "            index.add(proj_emb.reshape(1, -1))\n",
    "            id_to_doc[global_idx] = {\n",
    "                'type': 'text',\n",
    "                'paper_id': paper_id,\n",
    "                'path': pdf_path\n",
    "            }\n",
    "            global_idx += 1\n",
    "\n",
    "            # Image Processing\n",
    "            try:\n",
    "                images = extract_images_from_pdf(pdf_path, img_dir)\n",
    "                for img_path in images:\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    inputs = clip_processor(images=img, return_tensors=\"pt\")\n",
    "                    with torch.no_grad():\n",
    "                        img_emb = clip_model.get_image_features(**inputs).cpu().numpy()\n",
    "                    \n",
    "                    # Add to index\n",
    "                    index.add(img_emb.reshape(1, -1))\n",
    "                    id_to_doc[global_idx] = {\n",
    "                        'type': 'image',\n",
    "                        'paper_id': paper_id,\n",
    "                        'path': img_path\n",
    "                    }\n",
    "                    global_idx += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Image error {paper_id}: {e}\")\n",
    "\n",
    "        # Save incremental state\n",
    "        if (batch_idx // batch_size) % 10 == 0:\n",
    "            faiss.write_index(index, os.path.join(index_dir, f\"temp_index_{batch_idx}.index\"))\n",
    "            with open(os.path.join(index_dir, f\"temp_mapping_{batch_idx}.json\"), 'w') as f:\n",
    "                json.dump(id_to_doc, f)\n",
    "\n",
    "    # Final save\n",
    "    faiss.write_index(index, os.path.join(index_dir, \"final_index.index\"))\n",
    "    with open(os.path.join(index_dir, \"final_mapping.json\"), 'w') as f:\n",
    "        json.dump(id_to_doc, f)\n",
    "    torch.save(projection.state_dict(), os.path.join(index_dir, \"projection.pt\"))\n",
    "    \n",
    "    return index, id_to_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "index, mapping = process_papers(\n",
    "    metadata=load_metadata(\"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\"), \n",
    "    output_base=\"/kaggle/working\",\n",
    "    num_papers=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArXivRetriever:\n",
    "    def __init__(self, index_path, mapping_path, projection_path):\n",
    "        \"\"\"\n",
    "        Initialize the retriever.\n",
    "        \n",
    "        Args:\n",
    "            index_path (str): Path to the FAISS index.\n",
    "            mapping_path (str): Path to the id_to_doc mapping JSON file.\n",
    "            projection_path (str): Path to the projection layer weights.\n",
    "        \"\"\"\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Load id_to_doc mapping\n",
    "        with open(mapping_path) as f:\n",
    "            self.id_to_doc = json.load(f)\n",
    "        \n",
    "        # Load projection layer\n",
    "        self.projection = ProjectionLayer(384, 512)\n",
    "        self.projection.load_state_dict(torch.load(projection_path))\n",
    "        self.projection.eval()\n",
    "        \n",
    "        # Load text embedding model\n",
    "        self.text_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        \"\"\"\n",
    "        Embed and project the query text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The query text.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: The projected query embedding.\n",
    "        \"\"\"\n",
    "        \n",
    "        query_embedding = self.text_encoder.encode(text)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.projection(torch.tensor(query_embedding)).numpy()\n",
    "        # Ensure normalization\n",
    "        faiss.normalize_L2(query_embedding.reshape(1, -1))  # <-- Critical fix\n",
    "        return query_embedding\n",
    "    \n",
    "    def query(self, text, k=10):\n",
    "        \"\"\"\n",
    "        Retrieve relevant text and image results for a query.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The query text.\n",
    "            k (int): Number of results to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            List[dict]: A list of results, each containing:\n",
    "                - type: \"text\" or \"image\"\n",
    "                - paper_id: The arXiv paper ID\n",
    "                - path: Path to the PDF or image\n",
    "                - score: Similarity score\n",
    "                - text: Text content (for text results)\n",
    "            \"\"\"\n",
    "        # Embed the query\n",
    "        query_embedding = self.embed_query(text)\n",
    "        \n",
    "        # Reshape query_embedding to 2D (1 query, embedding_dimension)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Search the FAISS index\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Extract relevant metadata\n",
    "        results = []\n",
    "        for idx, dist in zip(indices[0], distances[0]):\n",
    "            if str(idx) in self.id_to_doc:\n",
    "                doc = self.id_to_doc[str(idx)]\n",
    "                result = {\n",
    "                    'type': doc['type'],\n",
    "                    'paper_id': doc['paper_id'],  # Use 'paper_id' instead of 'paper'\n",
    "                    #'path': doc['path'],\n",
    "                    'score': float(dist),\n",
    "                }\n",
    "                if doc['type'] == 'text':\n",
    "                    result['text'] = doc.get('text', '')  # Add text content for text results\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "       \n",
    "    def display_results(self, query, k=10):\n",
    "        \"\"\"\n",
    "        Display both text and image results for a query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The query text.\n",
    "            k (int): Number of results to retrieve.\n",
    "        \"\"\"\n",
    "        # Retrieve results\n",
    "        results = self.query(query, k)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"Results for query: '{query}'\\n\")\n",
    "        for res in results:\n",
    "            if res['type'] == 'text':\n",
    "                print(f\"Text from Paper {res['paper_id']} (Score: {res['score']:.4f}):\")\n",
    "                print(res.get('text', ''))  # Display text content\n",
    "                print(f\"PDF Path: {res['path']}\\n\")\n",
    "            elif res['type'] == 'image':\n",
    "                print(f\"Image from Paper {res['paper_id']} (Score: {res['score']:.4f}):\")\n",
    "                print(f\"Image Path: {res['path']}\\n\")\n",
    "\n",
    "    def query_images(self, text, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve only image results.\n",
    "        \"\"\"\n",
    "        results = self.query(text, k=k)\n",
    "        print(\"All results (text + images):\")\n",
    "        for res in results[:10]:  # Print top 10 for inspection\n",
    "            print(f\"Type: {res['type']}, Score: {res['score']:.2f}\")\n",
    "        image_results = [res for res in results if res['type'] == 'image']\n",
    "        return image_results\n",
    "    \n",
    "    def query_texts(self, text, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve only text results.\n",
    "        \"\"\"\n",
    "        results = self.query(text, k=k)\n",
    "        text_results = [res for res in results if res['type'] == 'text']\n",
    "        return text_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Indexing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m index, mapping \u001b[38;5;241m=\u001b[39m process_papers(\n\u001b[1;32m----> 3\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[43mload_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \n\u001b[0;32m      4\u001b[0m     output_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     num_papers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Retrieval\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retriever \u001b[38;5;241m=\u001b[39m ArXivRetriever(\n\u001b[0;32m     10\u001b[0m     index_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/faiss_index/final_index.index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     mapping_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/faiss_index/final_mapping.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     projection_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/faiss_index/projection.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m, in \u001b[0;36mload_metadata\u001b[1;34m(metadata_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_metadata\u001b[39m(metadata_path):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Load arXiv metadata from the JSON file.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m        List[dict]: List of paper metadata.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrieval\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = retriever.query(\"transformer neural networks\", k=5)\n",
    "for res in results:\n",
    "    print(f\"{res['type'].upper()} from {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:27:13.665709Z",
     "iopub.status.busy": "2025-03-15T18:27:13.665396Z",
     "iopub.status.idle": "2025-03-15T18:27:15.844727Z",
     "shell.execute_reply": "2025-03-15T18:27:15.844021Z",
     "shell.execute_reply.started": "2025-03-15T18:27:13.665687Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-32f8a352a21c>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.projection.load_state_dict(torch.load(projection_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bb8756156f463e8f405f56a5de8f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT from 0704.0202 (Score: 0.34)\n",
      "Path: /kaggle/working/pdfs/0704.0202.pdf\n",
      "\n",
      "TEXT from 0704.0323 (Score: 0.44)\n",
      "Path: /kaggle/working/pdfs/0704.0323.pdf\n",
      "\n",
      "TEXT from 0704.0268 (Score: 0.47)\n",
      "Path: /kaggle/working/pdfs/0704.0268.pdf\n",
      "\n",
      "TEXT from 0704.0482 (Score: 0.47)\n",
      "Path: /kaggle/working/pdfs/0704.0482.pdf\n",
      "\n",
      "TEXT from 0704.0051 (Score: 0.49)\n",
      "Path: /kaggle/working/pdfs/0704.0051.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = retriever.query(\"Quantum computing\", k=5)\n",
    "for res in results:\n",
    "    print(f\"{res['type'].upper()} from {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:28:01.311054Z",
     "iopub.status.busy": "2025-03-15T18:28:01.310696Z",
     "iopub.status.idle": "2025-03-15T18:28:03.648015Z",
     "shell.execute_reply": "2025-03-15T18:28:03.647203Z",
     "shell.execute_reply.started": "2025-03-15T18:28:01.311012Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-32f8a352a21c>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.projection.load_state_dict(torch.load(projection_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0e81514d77454885b0120e5bf4fb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT from 0704.0010 (Score: 0.68)\n",
      "Path: /kaggle/working/pdfs/0704.0010.pdf\n",
      "\n",
      "TEXT from 0704.0379 (Score: 0.68)\n",
      "Path: /kaggle/working/pdfs/0704.0379.pdf\n",
      "\n",
      "TEXT from 0704.0051 (Score: 0.73)\n",
      "Path: /kaggle/working/pdfs/0704.0051.pdf\n",
      "\n",
      "TEXT from 0704.0026 (Score: 0.73)\n",
      "Path: /kaggle/working/pdfs/0704.0026.pdf\n",
      "\n",
      "TEXT from 0704.0112 (Score: 0.77)\n",
      "Path: /kaggle/working/pdfs/0704.0112.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = retriever.query(\"Diagram\", k=5)\n",
    "for res in results:\n",
    "    print(f\"{res['type'].upper()} from {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:28:32.510027Z",
     "iopub.status.busy": "2025-03-15T18:28:32.509701Z",
     "iopub.status.idle": "2025-03-15T18:28:34.662531Z",
     "shell.execute_reply": "2025-03-15T18:28:34.661790Z",
     "shell.execute_reply.started": "2025-03-15T18:28:32.509999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-32f8a352a21c>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.projection.load_state_dict(torch.load(projection_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee0763b96064fd399cf45990793ebcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT from 0704.0445 (Score: 0.59)\n",
      "Path: /kaggle/working/pdfs/0704.0445.pdf\n",
      "\n",
      "TEXT from 0704.0271 (Score: 0.63)\n",
      "Path: /kaggle/working/pdfs/0704.0271.pdf\n",
      "\n",
      "TEXT from 0704.0107 (Score: 0.69)\n",
      "Path: /kaggle/working/pdfs/0704.0107.pdf\n",
      "\n",
      "TEXT from 0704.0444 (Score: 0.69)\n",
      "Path: /kaggle/working/pdfs/0704.0444.pdf\n",
      "\n",
      "TEXT from 0704.0131 (Score: 0.70)\n",
      "Path: /kaggle/working/pdfs/0704.0131.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = retriever.query(\"Model\", k=5)\n",
    "for res in results:\n",
    "    print(f\"{res['type'].upper()} from {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:13:14.365148Z",
     "iopub.status.busy": "2025-03-15T18:13:14.364759Z",
     "iopub.status.idle": "2025-03-15T18:13:14.385721Z",
     "shell.execute_reply": "2025-03-15T18:13:14.384850Z",
     "shell.execute_reply.started": "2025-03-15T18:13:14.365117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2192 embeddings to /kaggle/working/embeddings/faiss_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_embeddings_from_faiss(faiss_index, output_folder):\n",
    "    \"\"\"\n",
    "    Save embeddings from a FAISS index to disk.\n",
    "    \n",
    "    Args:\n",
    "        faiss_index (faiss.Index): The FAISS index containing embeddings.\n",
    "        output_folder (str): Folder to save the embeddings.\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get the total number of embeddings in the index\n",
    "    num_embeddings = faiss_index.ntotal\n",
    "    \n",
    "    # Initialize a list to store embeddings\n",
    "    embeddings = []\n",
    "    \n",
    "    # Extract embeddings from the FAISS index\n",
    "    for idx in range(num_embeddings):\n",
    "        embedding = faiss_index.reconstruct(idx)  # Reconstruct embedding by ID\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Convert embeddings to a NumPy array\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    \n",
    "    # Save embeddings to disk\n",
    "    embeddings_path = os.path.join(output_folder, \"faiss_embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings_array)\n",
    "    print(f\"Saved {num_embeddings} embeddings to {embeddings_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the FAISS index\n",
    "    faiss_index_path = \"/kaggle/working/faiss_index/final_index.index\"\n",
    "    faiss_index = faiss.read_index(faiss_index_path)\n",
    "    \n",
    "    # Save embeddings\n",
    "    save_embeddings_from_faiss(faiss_index, \"/kaggle/working/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:14:12.840200Z",
     "iopub.status.busy": "2025-03-15T18:14:12.839827Z",
     "iopub.status.idle": "2025-03-15T18:14:21.065844Z",
     "shell.execute_reply": "2025-03-15T18:14:21.064956Z",
     "shell.execute_reply.started": "2025-03-15T18:14:12.840170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['zip', '-r', '/kaggle/working/output__.zip', '/kaggle/working/'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"zip\", \"-r\", \"/kaggle/working/output__.zip\", \"/kaggle/working/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:17:46.238761Z",
     "iopub.status.busy": "2025-03-15T18:17:46.238439Z",
     "iopub.status.idle": "2025-03-15T18:17:46.284202Z",
     "shell.execute_reply": "2025-03-15T18:17:46.283443Z",
     "shell.execute_reply.started": "2025-03-15T18:17:46.238738Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"/kaggle/working/output__.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:36:48.586479Z",
     "iopub.status.busy": "2025-03-15T18:36:48.586170Z",
     "iopub.status.idle": "2025-03-15T18:36:50.686936Z",
     "shell.execute_reply": "2025-03-15T18:36:50.686270Z",
     "shell.execute_reply.started": "2025-03-15T18:36:48.586455Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-b322966d708e>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.projection.load_state_dict(torch.load(projection_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b3fc9bb62046de8534e9d8999f7872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT from 0704.0484 (Score: 0.72)\n",
      "Path: /kaggle/working/pdfs/0704.0484.pdf\n",
      "\n",
      "TEXT from 0704.0128 (Score: 0.76)\n",
      "Path: /kaggle/working/pdfs/0704.0128.pdf\n",
      "\n",
      "TEXT from 0704.0051 (Score: 0.78)\n",
      "Path: /kaggle/working/pdfs/0704.0051.pdf\n",
      "\n",
      "TEXT from 0704.0348 (Score: 0.80)\n",
      "Path: /kaggle/working/pdfs/0704.0348.pdf\n",
      "\n",
      "TEXT from 0704.0261 (Score: 0.80)\n",
      "Path: /kaggle/working/pdfs/0704.0261.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = retriever.query(\"Plot\", k=5)\n",
    "for res in results:\n",
    "    print(f\"{res['type'].upper()} from {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:39:07.172948Z",
     "iopub.status.busy": "2025-03-15T18:39:07.172633Z",
     "iopub.status.idle": "2025-03-15T18:39:09.276826Z",
     "shell.execute_reply": "2025-03-15T18:39:09.275357Z",
     "shell.execute_reply.started": "2025-03-15T18:39:07.172921Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-b322966d708e>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.projection.load_state_dict(torch.load(projection_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43b514487ba4761be9de17a4642260b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All results (text + images):\n",
      "Type: text, Score: 0.63\n",
      "Type: text, Score: 0.73\n",
      "Type: text, Score: 0.75\n",
      "Type: text, Score: 0.76\n",
      "Type: text, Score: 0.76\n",
      "Type: text, Score: 0.77\n",
      "Type: text, Score: 0.77\n",
      "Type: text, Score: 0.77\n",
      "Type: text, Score: 0.78\n",
      "Type: text, Score: 0.78\n",
      "Image results for query: 'tree'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the retriever\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query specifically for images\n",
    "image_query = \"tree\"\n",
    "image_results = retriever.query_images(image_query, k=50)\n",
    "\n",
    "# Display image results\n",
    "print(f\"Image results for query: '{image_query}'\\n\")\n",
    "for res in image_results:\n",
    "    print(f\"Image from Paper {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if Images Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:33:45.153228Z",
     "iopub.status.busy": "2025-03-15T18:33:45.152849Z",
     "iopub.status.idle": "2025-03-15T18:33:45.164238Z",
     "shell.execute_reply": "2025-03-15T18:33:45.163273Z",
     "shell.execute_reply.started": "2025-03-15T18:33:45.153198Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image entries: 1696\n",
      "Sample image entry: {'type': 'image', 'paper_id': '0704.0001', 'path': '/kaggle/working/images/page_15_img_1.jpeg'}\n",
      "Image exists: True\n"
     ]
    }
   ],
   "source": [
    "# Load id_to_doc mapping\n",
    "with open(\"/kaggle/working/faiss_index/final_mapping.json\", \"r\") as f:\n",
    "    id_to_doc = json.load(f)\n",
    "\n",
    "# Count image entries\n",
    "image_count = sum(1 for doc in id_to_doc.values() if doc[\"type\"] == \"image\")\n",
    "print(f\"Total image entries: {image_count}\")\n",
    "\n",
    "# Check if image paths exist\n",
    "sample_image_entry = next((doc for doc in id_to_doc.values() if doc[\"type\"] == \"image\"), None)\n",
    "if sample_image_entry:\n",
    "    print(\"Sample image entry:\", sample_image_entry)\n",
    "    print(\"Image exists:\", os.path.exists(sample_image_entry[\"path\"]))\n",
    "else:\n",
    "    print(\"No image entries found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-15T18:39:14.892084Z",
     "iopub.status.busy": "2025-03-15T18:39:14.891648Z",
     "iopub.status.idle": "2025-03-15T18:39:17.058705Z",
     "shell.execute_reply": "2025-03-15T18:39:17.057978Z",
     "shell.execute_reply.started": "2025-03-15T18:39:14.892024Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-b322966d708e>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.projection.load_state_dict(torch.load(projection_path))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7479976b82b4075bc3cf87c2fe0f045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT from 0704.0392 (Score: 0.71)\n",
      "Path: /kaggle/working/pdfs/0704.0392.pdf\n",
      "\n",
      "TEXT from 0704.0304 (Score: 0.82)\n",
      "Path: /kaggle/working/pdfs/0704.0304.pdf\n",
      "\n",
      "TEXT from 0704.0301 (Score: 0.86)\n",
      "Path: /kaggle/working/pdfs/0704.0301.pdf\n",
      "\n",
      "TEXT from 0704.0093 (Score: 0.87)\n",
      "Path: /kaggle/working/pdfs/0704.0093.pdf\n",
      "\n",
      "TEXT from 0704.0051 (Score: 0.87)\n",
      "Path: /kaggle/working/pdfs/0704.0051.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "retriever = ArXivRetriever(\n",
    "    index_path=\"/kaggle/working/faiss_index/final_index.index\",\n",
    "    mapping_path=\"/kaggle/working/faiss_index/final_mapping.json\",\n",
    "    projection_path=\"/kaggle/working/faiss_index/projection.pt\"\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = retriever.query(\"brain\", k=5)\n",
    "for res in results:\n",
    "    print(f\"{res['type'].upper()} from {res['paper_id']} (Score: {res['score']:.2f})\")\n",
    "    print(f\"Path: {res['path']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 10965686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
